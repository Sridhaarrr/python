{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047f9a7-85f2-44ed-96e9-a67157593ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pdf2docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d662bb1-1a90-4fcb-a48c-b2ef00ed7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import *\n",
    "tabulate(bis.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e4a6e-6da2-41ae-859e-da6ea31192f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "data = pd.read_excel('FAR.xlsx', 'Sheet1')\n",
    "data = data[data['Category2']=='CIRCULAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a9b72-d861-410c-8e19-30e0a9fc69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "d=[]\n",
    "# creating a pdf file object\n",
    "for i in data['Answer'][0:8]:\n",
    "    pdfFileObj = open(i, 'rb')\n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    # printing number of pages in pdf file\n",
    "    #print(pdfReader.numPages)\n",
    "    # creating a page object\n",
    "    pageObj = pdfReader.getPage(0)\n",
    "    bis = pageObj.extractText()\n",
    "    processed_text = re.sub(r'\\n', ' ', str(bis))\n",
    "    processed_text = re.sub(r' ', '', str(processed_text))\n",
    "    i = re.sub(r'\\W+', '', str(processed_text))\n",
    "    print(i)\n",
    "    try:\n",
    "        if 'INFPBG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'PBG(\\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'PBG(\\d+\\w?)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'PBG(\\d+\\w+\\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        if 'INFPBAG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'PBAG(\\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'PBAG(\\d+\\w?)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'PBAG(\\d+ \\w+ \\d+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "  \n",
    "        elif 'INFCBG' in i:\n",
    "        #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'CBG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'CBG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'CBG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INFPBLG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'PBLG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'PBLG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'PBLG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INFIBG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'IBG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'IBG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'IBG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INFCBD' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'CBD (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'CBD (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'CBD (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INFOD' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'OD (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'OD (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'OD (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        #else: d.append(' ')\n",
    "    except: d.append(' ')\n",
    "#     match = re.search(r'PBG(\\d+)',processed_text)\n",
    "#     match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b77e5-e08b-41dc-b360-8e059749cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '17 April 03 2021'\n",
    "s[3:len(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3112f3a-70f5-4508-a4f4-607d8a6cb47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser as dparser\n",
    "import pandas as pd\n",
    "data = pd.read_excel('tedate.xlsx', 'Sheet1')\n",
    "f=[]\n",
    "for s in data['d2']:\n",
    "    if s != ' ':\n",
    "        try:\n",
    "            f.append(dparser.parse(s[3:len(s)],fuzzy=True).strftime(\"%m-%d-%Y\"))\n",
    "        except ValueError:\n",
    "            f.append(' ')\n",
    "            #print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd612a0a-5456-4c4b-a131-2fa73514f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'value=\"((?:\\d+(?:\\.\\d+)?)?)', 'value=\"PERSONALBANKINGASSETSGROUPCENTRALOFFICEKARUR639002CircularNo11202021INFPBAG2January122021ROIConcessionVehicleLoansAfterthePostcovidLockdownreleaseourVehicleloansbusinesshasimprovedespeciallyourCarloanbusinessinQ32020OurcurrentVehicleloanbooksizehascrossedRs1000Crores2Wheelerand4WheelerandinQ3wehavedoneRs100CroresdisbursalinCarloansaloneConsideringthecurrentcompetitioninthemarketfromotherfinanciersandprovidefasterTATtothecustomersthebelowchangesinDelegationofpowerstotheDivisionalmanagersonprovidingROIconcessionwillhelpustoconvertmorecustomersandwillhelpusimproveourmonthlyCarloanvolumesinQ4RevisedNewpowerstobegivenattheDivisionalManagersLevelOnlyCarloansSrnoProfileofCustomerLoanamountInRsApprovalpowerstoDivisionalHeadsInterestConcession1BothSalariedandSelfemployed15LakhsUpto075forSelfEmployedUpto050forSalariedprofile2ForSalariedprofileAnyamountUpto0253ForSelfEmployedProfileAnyamountUpto050AnyROIconcessionwhichisbelowtotheabovementionedtablewillberoutedtothePBAGCOteamTheaboveapprovalpowerswillbevalidforaperiodof3monthsuntil31032021Thesamewillbereviewedafter31032021DivisionalofficesandBranchesarerequestedtomakeanoteoftheaboveASSTGENERALMANAGERGENERALMANAGERHIGHLIGHTSOFTHECIRCULAR\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5b975-a93a-4fb8-baf7-dcb0f0521180",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = \"PERSONALBANKINGASSETSGROUPCENTRALOFFICEKARUR639002CircularNo11202021INFPBAG2January122021ROIConcessionVehicleLoansAfterthePostcovidLockdownreleaseourVehicleloansbusinesshasimprovedespeciallyourCarloanbusinessinQ32020OurcurrentVehicleloanbooksizehascrossedRs1000Crores2Wheelerand4WheelerandinQ3wehavedoneRs100CroresdisbursalinCarloansaloneConsideringthecurrentcompetitioninthemarketfromotherfinanciersandprovidefasterTATtothecustomersthebelowchangesinDelegationofpowerstotheDivisionalmanagersonprovidingROIconcessionwillhelpustoconvertmorecustomersandwillhelpusimproveourmonthlyCarloanvolumesinQ4RevisedNewpowerstobegivenattheDivisionalManagersLevelOnlyCarloansSrnoProfileofCustomerLoanamountInRsApprovalpowerstoDivisionalHeadsInterestConcession1BothSalariedandSelfemployed15LakhsUpto075forSelfEmployedUpto050forSalariedprofile2ForSalariedprofileAnyamountUpto0253ForSelfEmployedProfileAnyamountUpto050AnyROIconcessionwhichisbelowtotheabovementionedtablewillberoutedtothePBAGCOteamTheaboveapprovalpowerswillbevalidforaperiodof3monthsuntil31032021Thesamewillbereviewedafter31032021DivisionalofficesandBranchesarerequestedtomakeanoteoftheaboveASSTGENERALMANAGERGENERALMANAGERHIGHLIGHTSOFTHECIRCULAR\"\n",
    "match = re.search(r'PBAG(\\d+\\w+\\d?)',i)\n",
    "match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd448a-6dc1-4f80-83bb-d3dbfa1895f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# processed_text = re.sub(r'\\W+', ' ', str(bis))\n",
    "# processed_text\n",
    "match = re.search(r'PBG(\\d+)',processed_text)\n",
    "match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0e5d1-4910-4b59-ae8d-19d7ac5abc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "d = []\n",
    "for i in data['STU']:\n",
    "    try:\n",
    "        if 'INF PBAG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'PBAG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'PBAG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'PBAG (\\d+ \\w+ \\d+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7d5b7-5303-487a-ba0b-ed93354239e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spaczz.matcher import FuzzyMatcher\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "text = \"\"\"What are the mandatory details required for creating the Digital retail application?\"\"\"  # Spelling errors intentional.\n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = FuzzyMatcher(nlp.vocab)\n",
    "matcher.add(\"PLACE\",[nlp(\"retail application mandatory DETAILS\")])\n",
    "#matcher.add([nlp(\"Nashville\")])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end, ratio in matches:\n",
    "    print(match_id, doc[start:end], ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af2662-8905-4e65-861f-29922fe87acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Primary applicant swap'\n",
    "a = []\n",
    "for i in data.iloc[:,1]:\n",
    "    d = fuzz.ratio(i,string)\n",
    "    a.append(d)in\n",
    "b = a.index(max(a))\n",
    "data.iloc[b,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0c974-1a88-445f-9a10-ab63804d6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['score'] = a\n",
    "df1 = data.sort_values('score',ascending = False).reset_index()#.groupby('pidx').head(2)\n",
    "#df1['Freqently asked questions']\n",
    "for i in range(5):\n",
    "    print(df1['Freqently asked questions'][i])\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad493a08-845e-4f56-a388-ed2667835733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74b603-2e16-4459-ae81-19a8090cbf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = data1['Freqently asked questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfd3c0-1980-4440-8df5-f6316926f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "nopunc = [char for char in e if char not in string.punctuation]\n",
    "nopunc = ''.join(nopunc)\n",
    "# Remove stopwords\n",
    "processed_text =[word for word in nopunc.split() if word.lower() not in nltk.corpus.stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8534a-e57d-4d2e-86a6-90ed96903f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nop = []\n",
    "for t in e:\n",
    "    nopunc = [char for char in t if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    nop.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec771e-8d07-4493-9d11-beb46db7fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nop1 = []\n",
    "for t in nop:\n",
    "    processed_text =[word for word in t.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    processed_text = ' '.join(processed_text)\n",
    "    nop1.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc26f14-b922-4e2a-8832-65c6d8e27056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gingerit.gingerit import GingerIt\n",
    "text = input(\"Enter a sentence >>: \")\n",
    "corrected_text = GingerIt().parse(text)\n",
    "print(corrected_text['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283fd50-e737-4e68-8c38-ad9bd39604c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from fuzzywuzzy import fuzz\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "data = pd.read_excel('FAQ_2.xlsx', 'Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e4c58a-ffcb-43a2-81c8-3cc926fd8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = input()\n",
    "l = [spell(i) for i in string.split()]\n",
    "#s= [word for word in l if word not in stopwords.words()]\n",
    "string = ' '.join(l)\n",
    "# string\n",
    "text_tokens = word_tokenize(string)\n",
    "s= [word for word in text_tokens if word not in stopwords.words()]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7c936-4ab1-4b8c-ab0e-6d9a239c1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = input()\n",
    "#l = [spell(i) for i in string.split()]\n",
    "text_tokens = word_tokenize(string)\n",
    "s= [word for word in text_tokens if word not in stopwords.words()]\n",
    "string = ' '.join(s)\n",
    "#string = 'error digital executed guarantee loan document'\n",
    "a = []\n",
    "for i in data.iloc[:,1]:\n",
    "    d = fuzz.partial_ratio(i,string)\n",
    "    a.append(d)\n",
    "b = a.index(max(a))\n",
    "data.iloc[b,1]\n",
    "data['score'] = a\n",
    "df1 = data.sort_values('score',ascending = False).reset_index()#.groupby('pidx').head(2)\n",
    "#df1['Freqently asked questions']\n",
    "for i in range(5):\n",
    "    print(df1['Freqently asked questions'][i],'\\n\\n', df1['Answer'][i])\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdf6be-fdd6-4474-91cd-29783a4bcb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2docx import Converter\n",
    "import os\n",
    "\n",
    "# # # dir_path for input reading and output files & a for loop # # #\n",
    "\n",
    "path_input = 'C:/Users/018598/Circulars/2022/'\n",
    "path_output = 'C:/Users/018598/Circular_docx/2022_docx/'\n",
    "\n",
    "for file in os.listdir(path_input):\n",
    "    cv = Converter(path_input+file)\n",
    "    cv.convert(path_output+file+'.docx', start=0, end=None)\n",
    "    cv.close()\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5897806-4060-4297-94df-f512b4846237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2docx import Converter\n",
    "import os\n",
    "cv = Converter(\"2022001HRDCR.pdf\")\n",
    "cv.convert('Fed1.docx', start=0, end=None)\n",
    "cv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6b7eb-0967-43f1-ad22-d1066861264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import docx\n",
    "nop = []\n",
    "for t in data.iloc[:,2]:\n",
    "    processed_text =[word for word in t.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    processed_text =[word for word in processed_text if word.lower() not in i.isdigit()]\n",
    "    processed_text = ' '.join(processed_text)\n",
    "    processed_text = re.sub(r'\\W+', ' ', str(processed_text))\n",
    "    nop.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c934128-d0a2-4d65-8ca7-ccc6e58ebedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "os.chdir(r\"C:/Users/018598/Documents/MT\")\n",
    "listvals = []\n",
    "for i in os.listdir(\"C:/Users/018598/Documents/MT\"):\n",
    "    if 'docx' in i:\n",
    "        st = getText(str(i))\n",
    "        res = \"\".join(filter(lambda x: not x.isdigit(), st))\n",
    "        processed_text =[word.lower() for word in st.split() if word.lower() not in nltk.corpus.stopwords.words('english') and word.isalpha()]\n",
    "        processed_text = ' '.join(processed_text)\n",
    "        listvals.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46ec2b2-4901-4304-a808-58580538fc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import docx\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d86458-726b-45d7-994d-f2fac71ee110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from datetime import datetime,date\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "#os.chdir(r\"C:\\Users\\018598\\Desktop\\KK\")\n",
    "listvals = []\n",
    "for i in os.listdir(\"C:/Users/018598/Documents/News_after\"):\n",
    "    if ('docx' in i):\n",
    "        #print(i)\n",
    "        st = getText(\"C:/Users/018598/Documents/News_after/\"+str(i))\n",
    "        if ('Article Title:' in st):\n",
    "            st = re.sub(r'\\t', '', str(st))\n",
    "            q = re.compile(r'\\nArticle Title:+(?:(?!var\\d).)*?\\nFull Article:', re.DOTALL)\n",
    "            data = q.findall(st)\n",
    "            processed_text = re.sub(r'Article Title:', '', str(data[0]))\n",
    "            processed_text = re.sub(r'Full Article:', '', str(processed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c693bc-3b1a-4143-ab72-5fe473d22f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "from datetime import datetime,date\n",
    "import nltk\n",
    "import re\n",
    "#os.chdir(r\"C:\\Users\\018598\\Desktop\\KK\")\n",
    "listvals = []\n",
    "for i in os.listdir(\"C:/Users/018598/Desktop/Malegam_7-1-22\"):\n",
    "    if ('docx' in i):\n",
    "        #print(i)\n",
    "        st = getText(\"C:/Users/018598/Desktop/Malegam_7-1-22/\"+str(i))\n",
    "        if ('Article Title:' in st):\n",
    "            st = re.sub(r'\\t', '', str(st))\n",
    "            q = re.compile(r'\\nArticle Title:+(?:(?!var\\d).)*?\\nFull Article:', re.DOTALL)\n",
    "            data = q.findall(st)\n",
    "            processed_text = re.sub(r'Article Title:', '', str(data[0]))\n",
    "            processed_text = re.sub(r'Full Article:', '', str(processed_text))\n",
    "            r = re.compile(r'Company:+(?:(?!var\\d).)*?\\nArticle URL:', re.DOTALL)\n",
    "            data1 = r.findall(st)\n",
    "            processed_text1 = re.sub(r'Company:', '', str(data1[0]))\n",
    "            processed_text1 = re.sub(r'Article URL:', '', str(processed_text1))\n",
    "            r = re.compile(r'\\nArticle URL:+(?:(?!var\\d).)*?\\nArticle Title:', re.DOTALL)\n",
    "            data2 = r.findall(st)\n",
    "            processed_text2 = re.sub(r'Article URL:', '', str(data2[0]))\n",
    "            processed_text2 = re.sub(r'Article Title:', '', str(processed_text2))\n",
    "\n",
    "        else:\n",
    "            q = re.compile(r'Title:\\n+(?:(?!var\\d).)*?\\n', re.DOTALL)\n",
    "            data = q.findall(st)\n",
    "            processed_text = re.sub(r'Title:\\n', '', str(data[0]))\n",
    "            processed_text = re.sub(r'\\n', '', str(processed_text))\n",
    "        processed_text = re.sub(r'\\n', '', str(processed_text))\n",
    "        processed_text1 = re.sub(r'\\n', '', str(processed_text1))\n",
    "        processed_text2 = re.sub(r'\\n', '', str(processed_text2))\n",
    "        listvals.append((i,processed_text1,datetime.fromtimestamp(os.path.getmtime(\"C:/Users/018598/Desktop/Malegam_7-1-22/\"+str(i))).date(),processed_text,processed_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "988d5d82-3f65-451e-8ea4-bda59d24d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(listvals).to_excel('Malegam2_2months.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3b595-f3d7-4fff-b975-c68c7d438830",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = getText(\"C:/Users/018598/Desktop/KK/AANCHAL ISPAT LTD _50_ Crores .docx\")\n",
    "st = re.sub(r'\\t', '', str(st))\n",
    "st\n",
    "# q = re.compile(r'\\nArticle Title:+(?:(?!var\\d).)*?\\nFull Article:', re.DOTALL)\n",
    "# data = q.findall(st)\n",
    "# processed_text = re.sub(r'\\nArticle Title: \\n', '', str(data[0]))\n",
    "# processed_text = re.sub(r'\\nFull Article:', '', str(processed_text))\n",
    "# processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394c572-2f01-43c1-9e48-c2ead4638984",
   "metadata": {},
   "outputs": [],
   "source": [
    "listvals = []\n",
    "c=0\n",
    "for i in os.listdir(\"C:/Users/018598/Desktop/KK\"):\n",
    "    if ('docx' in i):\n",
    "        #print(i)\n",
    "        st = getText(\"C:/Users/018598/Desktop/KK/\"+str(i))\n",
    "        if ('Article Title:' in st):\n",
    "            print(i)\n",
    "#             st = re.sub(r'\\t', '', str(st))\n",
    "#             q = re.compile(r'\\nArticle Title:+(?:(?!var\\d).)*?\\nFull Article:', re.DOTALL)\n",
    "#             data = q.findall(st)\n",
    "#             processed_text = re.sub(r'Article Title:', '', str(data[0]))\n",
    "#             processed_text = re.sub(r'Full Article:', '', str(processed_text))\n",
    "#             print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7ce2d-b724-459b-9b1c-e6d97c3ffa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "#os.chdir(r\"C:\\Users\\018598\\Desktop\\KK\")\n",
    "listvals = []\n",
    "for i in os.listdir(\"C:/Users/018598/Desktop/KK\"):\n",
    "    if (('docx' in i) and ('Article' in i)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84c912-483f-403a-9aa5-d8e9d8399f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = getText(\"C:/Users/018598/Desktop/KK/NORTHERN ARC CAPITAL LIMITED_50 Crores.docx\")\n",
    "#match = re.search(r'CBG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "#d.append(match.group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bcc8b-b7f7-46b4-9867-13a09ffdf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = ' '.join(listvals)\n",
    "key = set(lis.split())\n",
    "from english_words import english_words_set\n",
    "ki = [w for w in key if w.lower() in english_words_set or not w.isalpha()]\n",
    "ne = [w for w in key if w.lower() in nltk.corpus.stopwords.words('tajik') or not w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa52b277-8b21-451b-b6f8-2e8f79609f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyinflect\n",
    "from pyinflect import getAllInflections, getInflection\n",
    "ty=[]\n",
    "for j in nltk.corpus.stopwords.words('tajik'):\n",
    "    gt = [getAllInflections(j)[i][0] for i in getAllInflections(j).keys()]\n",
    "    ty+=gt\n",
    "ty = set(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb00852-85d5-4c30-b4d5-b0d0ae8112c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "(pd.DataFrame(ty)).to_excel('circul.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3afc9-005c-44f1-bd13-d8396d521d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "listvals = []\n",
    "for i in links:\n",
    "    listvals.append((re.findall(r'Circular No.(.*?)INF', st), re.findall(r'\\n \\n \\n(.*?)\\n \\n \\n', st), st))\n",
    "df = pd.DataFrame(listvals, columns=['Company','Redirect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30b109-0511-4c21-9e89-7206e3c7ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.Category2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b2286-d0f0-4225-a9d6-6cbacd559147",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data1[data1['Category1'] == data1.Category1.unique()[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad0f18-2252-41d4-ae71-7d329235dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = input()\n",
    "l = [spell(i) for i in string.split()]\n",
    "' '.join(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
