{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ce80b6-64f2-4146-b6d3-cefa63d8a94f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ee4f25e2f7ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnop1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Category2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'CIRCULAR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprocessed_text\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprocessed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nop1 = []\n",
    "c = 0\n",
    "for t in data[data['Category2']=='CIRCULAR'].iloc[:,2]:\n",
    "    processed_text =[word for word in t.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    processed_text = ' '.join(processed_text)\n",
    "    processed_text = re.sub(r'\\W+', ' ', str(processed_text))\n",
    "    c=c+1\n",
    "    if ('INF' in processed_text) or ('POL' in processed_text):\n",
    "        nop1.append(processed_text)\n",
    "    #processed_text = ' '.join(e for e in processed_text.split() if e.isalnum())\n",
    "    else: nop1.append(data[data['Category2']=='CIRCULAR']['Category1'].reset_index().iloc[c-1,1]+' '+processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6beaaad-8fe1-4b71-adf9-eaf910a399fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "nop = []\n",
    "for t in data.iloc[:,2]:\n",
    "    processed_text =[word for word in t.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    processed_text = ' '.join(processed_text)\n",
    "    processed_text = re.sub(r'\\W+', ' ', str(processed_text))\n",
    "    nop.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80bc2555-9570-41b9-8d7f-7dcd6f8e0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['STU'] = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40566a0d-d3b7-425f-ace5-5a3cb8c80e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Category2']=='CIRCULAR', 'STU'] = nop1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd76a77c-a4af-40f6-8578-9b5470a94aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('FAR.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe70871-ba5f-4063-9451-112acbefcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "path_input = 'C:/Users/018598/Circulars/'\n",
    "for t in data[data['Category2']=='CIRCULAR'].iloc[:,4]:\n",
    "    d.append(path_input+t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34799794-f3d5-4f6f-bd35-782c485341ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "nop1 = []\n",
    "c = 0\n",
    "for t in data[data['Category2']=='CIRCULAR'].iloc[:,1]:\n",
    "    processed_text =[word for word in t.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "    processed_text = ' '.join(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84130f39-c208-4e1b-95a7-df0104ff523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncommon(A, B):\n",
    "    un_comm = [i for i in \"\".join(B).split() if i not in \"\".join(A).split()]\n",
    "    return un_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9ddc4-4a5e-4b0c-a0f5-07bd5a44bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(len(data['STU'])):\n",
    "    bad_df = data['STU'].index.isin([i])\n",
    "    b = ' '.join((data['STU'][~bad_df]))\n",
    "    a = data['STU'][i]\n",
    "    c.append(uncommon(a,b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = input()\n",
    "#l = [spell(i) for i in string.split()]\n",
    "text_tokens = word_tokenize(string)\n",
    "s= [word for word in text_tokens if word not in stopwords.words()]\n",
    "string = ' '.join(s)\n",
    "#string = 'error digital executed guarantee loan document'\n",
    "a = []\n",
    "for i in data.iloc[:,5]:\n",
    "    d = fuzz.partial_ratio(str(i),str(string))\n",
    "    a.append(d)\n",
    "b = a.index(max(a))\n",
    "data.iloc[b,1]\n",
    "data['score'] = a\n",
    "df1 = data.sort_values('score',ascending = False).reset_index()#.groupby('pidx').head(2)\n",
    "#df1['Freqently asked questions']\n",
    "for i in range(5):\n",
    "    (df1['Freqently asked questions'][i]),'\\n\\n', df1['Answer'][i])\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e756488f-6bd5-4c4f-81ed-fca3d8656891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from fuzzywuzzy import fuzz\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('stopwords')\n",
    "spell = Speller(lang='en')\n",
    "#data = pd.read_excel('FAQ_2.xlsx', 'Sheet1')\n",
    "data = pd.read_excel('FAR.xlsx', 'Sheet1')\n",
    "#data  = data[data['Category2']=='LOS']\n",
    "# data  = data[data['Category2']=='SME']\n",
    "# data  = data[data['Category2']=='PBG']\n",
    "#data  = data[data['Category2']=='CIRCULAR']\n",
    "data = data[data['Category2']=='CIRCULAR']\n",
    "#data = data[~data['Date'].isna()]\n",
    "#data = data.sort_values('Date',ascending = False).reset_index().sort_values('Category1',ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dfe44c-9c3b-47e7-9fd9-461b6443bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = input()\n",
    "#l = [spell(i) for i in string.split()]\n",
    "text_tokens = word_tokenize(string)\n",
    "s= [word for word in text_tokens if word not in stopwords.words()]\n",
    "string = ' '.join(s)\n",
    "#string = 'error digital executed guarantee loan document'\n",
    "a = []\n",
    "for i in data.iloc[:,6]:\n",
    "    d = fuzz.partial_ratio(str(i),str(string))\n",
    "    a.append(d)\n",
    "data['score'] = a\n",
    "df1 = data.sort_values('score',ascending = False).reset_index().sort_values('Date',ascending = False).reset_index()#.groupby('pidx').head(2)\n",
    "#df1['Freqently asked questions']\n",
    "b = []\n",
    "for i in range(10):\n",
    "    if i<5:print(df1['Answer'][i])#,'\\n\\n', df1['Answer'][i])\n",
    "    b.append((df1['Freqently asked questions'][i], df1['Answer'][i], df1['Date'][i]))\n",
    "c=[]\n",
    "data1 = pd.DataFrame(b,columns=['Freqently asked questions', 'Answer', 'Date'])    \n",
    "for i in data1.iloc[:,0]:\n",
    "    d = fuzz.partial_ratio(str(i),str(string))\n",
    "    c.append(d)\n",
    "#list(data1.iloc[:,0])\n",
    "print('\\n\\n')\n",
    "\n",
    "data1['score'] = c\n",
    "df2 = data1.sort_values('score',ascending = False).reset_index().sort_values('Date',ascending = False).reset_index()\n",
    "for i in range(5):\n",
    "    print(df2['Answer'][i])#,'\\n\\n', df1['Answer'][i])\n",
    "#     #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abf0ab99-fbf1-4fd8-a75d-d103ba410802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " mrris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/018598/Circulars/290-2021 MG Scheme.pdf\n",
      "C:/Users/018598/Circulars/276-2021- Personal loan Relaunch.pdf\n",
      "C:/Users/018598/Circulars/206-2021 - ROI Home loan.pdf\n"
     ]
    }
   ],
   "source": [
    "string = input()\n",
    "#string = '130'\n",
    "text_tokens = word_tokenize(string)\n",
    "s= [word for word in text_tokens if word not in stopwords.words()]\n",
    "string = ' '.join(s)\n",
    "string = word_tokenize(string.lower())\n",
    "string\n",
    "#processed_text =[word for word in t.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
    "#string = 'error digital executed guarantee loan document'\n",
    "a = []\n",
    "for i in (range(len(data.iloc[:,6]))):\n",
    "    s = [word for word in string if word in data.iloc[i,6].lower()]\n",
    "    st = ' '.join(s)\n",
    "    a.append((st, data.iloc[i,3], data.iloc[i,7]))\n",
    "    \n",
    "a = pd.DataFrame(a, columns =[0,\"link\",\"date\"])\n",
    "b = [len(w.split()) for w in a[0]]\n",
    "a['score'] = b\n",
    "#a = a.sort_values('date',ascending = False)#.head(5) \n",
    "a = a.sort_values(['score', 'date'],ascending = False).head(3)\n",
    "f = [print(i) for i in a['link']]   \n",
    "    #d = fuzz.partial_ratio(str(i),str(string))\n",
    "    #a.append(d)\n",
    "# data['score'] = a\n",
    "# df1 = data.sort_values('score',ascending = False).reset_index().sort_values('Date',ascending = False).reset_index()#.groupby('pidx').head(2)\n",
    "# #df1['Freqently asked questions']\n",
    "# b = []\n",
    "# for i in range(10):\n",
    "#     if i<5:print(df1['Answer'][i])#,'\\n\\n', df1['Answer'][i])\n",
    "#     b.append((df1['Freqently asked questions'][i], df1['Answer'][i], df1['Date'][i]))\n",
    "# c=[]\n",
    "# data1 = pd.DataFrame(b,columns=['Freqently asked questions', 'Answer', 'Date'])    \n",
    "# for i in data1.iloc[:,0]:\n",
    "#     d = fuzz.partial_ratio(str(i),str(string))\n",
    "#     c.append(d)\n",
    "# #list(data1.iloc[:,0])\n",
    "# print('\\n\\n')\n",
    "# data1['score'] = c\n",
    "# df2 = data1.sort_values('score',ascending = False).reset_index().sort_values('Date',ascending = False).reset_index()\n",
    "# for i in range(5):\n",
    "#     print(df2['Answer'][i])#,'\\n\\n', df1['Answer'][i])\n",
    "# #     #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e791e26-3007-4cb3-8169-38210e96fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.sort_values('date',ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397b3b1-257f-4eec-968b-c84c4329d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = input()\n",
    "#l = [spell(i) for i in string.split()]\n",
    "text_tokens = word_tokenize(string)\n",
    "s= [word for word in text_tokens if word not in stopwords.words()]\n",
    "string = ' '.join(s)\n",
    "#string = 'error digital executed guarantee loan document'\n",
    "a = []\n",
    "for i in data.iloc[:,6]:\n",
    "    d = fuzz.partial_ratio(str(i),str(string))\n",
    "    a.append(d)\n",
    "data['score'] = a\n",
    "df1 = data.sort_values('score',ascending = False).reset_index()#.groupby('pidx').head(2)\n",
    "#df1['Freqently asked questions']\n",
    "b = []\n",
    "for i in range(10):\n",
    "    if i<5:print(df1['Answer'][i])#,'\\n\\n', df1['Answer'][i])\n",
    "    b.append((df1['Freqently asked questions'][i], df1['Answer'][i]))\n",
    "c=[]\n",
    "data1 = pd.DataFrame(b,columns=['Freqently asked questions', 'Answer'])    \n",
    "for i in data1.iloc[:,0]:\n",
    "    d = fuzz.ratio(str(i),str(string))\n",
    "    c.append(d)\n",
    "#list(data1.iloc[:,0])\n",
    "print('\\n\\n')\n",
    "\n",
    "data1['score'] = c\n",
    "df2 = data1.sort_values('score',ascending = False).reset_index()\n",
    "for i in range(5):\n",
    "    print(df2['Answer'][i])#,'\\n\\n', df1['Answer'][i])\n",
    "#     #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcc0feba-915d-4678-a00d-58ef90e23087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "d = []\n",
    "for i in data['STU']:\n",
    "    try:\n",
    "        if 'INF PBAG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'PBAG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'PBAG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'PBAG (\\d+ \\w+ \\d+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INF PBG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'PBG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'PBG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'PBG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INF CBG' in i:\n",
    "        #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'CBG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'CBG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'CBG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INF PBLG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'PBLG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'PBLG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'PBLG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INF IBG' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'IBG (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'IBG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'IBG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INF CBD' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'CBD (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'CBD (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'CBD (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        elif 'INF OD' in i:\n",
    "            #print(i, '\\n\\n')\n",
    "            try:\n",
    "                match = re.search(r'OD (\\d+ \\d+ \\d+ \\d+)',i)\n",
    "                d.append(match.group(1))\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    match = re.search(r'OD (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "                except AttributeError:\n",
    "                    match = re.search(r'OD (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "                    d.append(match.group(1))\n",
    "        else: d.append(' ')\n",
    "    except: d.append(' ')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13e350bf-cc33-425d-b1f6-a77d28b505f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['d2']=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92b45d32-50f1-44f2-969c-c6376b57d5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00d5816c-d69f-4dab-ae3e-8ff4c5cfd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,5:9].to_excel('tedate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a9cbe4d-4c5e-4f3b-aa3d-b0bcd7b92aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = []\n",
    "for i in d:\n",
    "    p = [k for k in i.split()[1:4][::-1]]\n",
    "    e.append(''.join(p))\n",
    "data['V'] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f4be254-2542-4cd7-9696-82d117f6aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('V',ascending = True).reset_index().to_excel('gos.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e056980-690f-4109-9fbe-b43d924f901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "{month: index for index, month in enumerate(calendar.month_abbr) if 'Jan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20559324-4b23-402a-8dc4-2eabcd51079f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13 19th May 2020'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = re.search(r'PBAG (\\d+ \\d+\\w+ \\w+ \\d+)','Circular 166 2020 21 INF PBAG 13 19th May 2020 continuation earlier Circular No 389 2017 18 dated 13 09 2017 Circular No 489 2018 19 Bank reviewed Education loan product view scope business opportunity segment Board permitted enhance Education Loan quantum studies India following modification existing Education loan scheme 01 02 03 Branches advised refer circular no 603 2016 dated 21 12 2016 per said circular advised students approach us education loan Rs 4 00 lacs studies abroad required make deposit Rs 5000 adjusted contribution margin money interest payable loan case loan availed applicant applicant avail loan within period 4 months sanction loan amount forfeited advise henceforth branches need collect said deposit amount Rs 5000 reiterate branches collect processing charges Education loans sanctioned per existing guidelines 04 Branches advised refer Circular no 60 18 INF PBG 15 dated 7 2 2018 wherein given salient features Credit Guarantee Fund Scheme Education loans CGFSEL note branches covering Education loans said Scheme limits upto Rs 7 50 lakhs advise branches Education loans upto Rs 7 50 lakhs shall compulsorily covered Credit Guarantee Fund Scheme subject condition collateral security third party guarantee attached loan deviation regard viewed seriously 05 also wish reiterate Holiday period stipulated Retail Credit Policy as per RBI guidelines strictly adhered to i e say Course Period plus one year terms conditions followed per existing IBA Model Education Loan Scheme ASST GENERAL MANAGER GENERAL MANAGER')\n",
    "match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ea08f982-326c-432c-b2a4-c0f12e8851b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in data['L']:\n",
    "    try:\n",
    "        match = re.search(r'PBAG (\\d+ \\d+\\w+ \\w+ \\d+)',i)\n",
    "        d.append(match.group(1))\n",
    "    except AttributeError:\n",
    "        match = re.search(r'PBAG (\\d+ \\w+ \\d+ \\d+)',i)\n",
    "        d.append(match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91187037-1182-449e-acf4-c36e7c8b8b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LOS', 'SME', 'PBG', 'CIRCULAR'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = {\n",
    "    'Number': ['1','2','3','4','5','6'],\n",
    "    'Category': ['Application creation and modification','Front End Tab related query','System Issues and Errors',\n",
    "                 'Perfios related query','Limit and Exceeding','CLS related query']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a92bbc99-3186-4422-8cea-6571a13d482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncommon(A, B):\n",
    "    un_comm = [i for i in \"\".join(B).split() if i not in \"\".join(A).split()]\n",
    "    return un_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865464a0-dd2a-4e21-bcf1-eb0f2f4ac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = \"Geeks for Geeks \" \n",
    "B = \"Learning from Geeks for the Geeks\"\n",
    "print(uncommon(A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bb1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=cat_list.loc[cat_list['Number'] == input(), 'Category'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaca444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df1[df1['Category']== cat]\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422303ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input()\n",
    "def get_ratio(row):\n",
    "    name1 = row['Freqently asked questions']\n",
    "    return fuzz.token_sort_ratio(name, name1)\n",
    "ik=df[df.apply(get_ratio, axis=1) == max(df.apply(get_ratio, axis=1))]\n",
    "ik['Answer'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036ffb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "#from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "driver = webdriver.Chrome(executable_path=r'chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1870e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr= '018598' #input('Enter Email Id:') \n",
    "pwd= 'Sridhar@19'#input('Enter Password:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d734037",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.frs.com/')\n",
    "print (\"Opened PBG\")\n",
    "sleep(1)\n",
    "#html = driver.page_source\n",
    "#username_box = driver.find_element_by_name('a')\n",
    "# username_box.send_keys(usr)\n",
    "# print (\"Email Id entered\")\n",
    "# sleep(1)\n",
    "# password_box = driver.find_element_by_name('password')\n",
    "# password_box.send_keys(pwd)\n",
    "# print (\"Password entered\")\n",
    "# login_box = driver.find_element_by_id('submit')\n",
    "# login_box.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c717c-c709-4675-be44-e2ba599f067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002ae580-0e60-4bab-a0bf-192b72f4e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(string):\n",
    "    pattern = re.compile(r'\\s+')\n",
    "    return re.sub(pattern, '%20', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6c627-309e-441a-9d95-eb6c7e04f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "d = re.findall(r'href=\"(.*?).pdf\"', html)\n",
    "links = []\n",
    "for i in d:\n",
    "    e='https://'+'www.kvbpbg.com/'+i+'.pdf'\n",
    "    links.append(remove(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e6c5c-db77-400f-8681-ffa30fb0f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "listvals = []\n",
    "for i in links:\n",
    "    listvals.append((re.findall(r'.com/(.*?).pdf', i),i))\n",
    "df = pd.DataFrame(listvals, columns=['Company','Redirect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0f919-ee01-4220-bb8c-442d8e27d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Links.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3383c-debd-4153-a36d-147c2b5ae7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(links[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47faa34b-436e-473a-9f36-9736a525b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "response = urllib2.urlopen(\"https://www.kvbpbg.com/Para/LIFE%20BSLI/FAQ/BSLI%20Income%20Assured%20Plan%20-%20FAQ's%20v%201%201_clean.pdf\")\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d19bdbf-8b84-406b-af7e-4fb0ee3512b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib3\n",
    "http = urllib3.PoolManager()\n",
    "r = http.request('GET', \"https://www.planetebook.com/free-ebooks/pride-and-prejudice.pdf\")\n",
    "r.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb8172-69af-45b4-bcf1-406d3291000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "pdf_path = \"https://www.planetebook.com/free-ebooks/pride-and-prejudice.pdf\"\n",
    "def download_file(download_url, filename):\n",
    "    response = urllib.request.urlopen(download_url)    \n",
    "    file = open(filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    " \n",
    "download_file(pdf_path, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2233dc8-29f9-46f1-b4c9-e31847d1664c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbbbf3-5d5b-4649-843f-9d6f5b106170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
